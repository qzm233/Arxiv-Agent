{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9b0aa2dd-e825-496b-9fec-12d25f2a24b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 打开JSON文件并将其解析为Python字典\n",
    "# with open('verification.json', 'r') as file:\n",
    "data = pd.read_json('verification.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a48b61d5-c130-44ed-8b08-0723a8092e18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>link</th>\n",
       "      <th>abstract</th>\n",
       "      <th>subjects</th>\n",
       "      <th>authors</th>\n",
       "      <th>published</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Identity Inference from CLIP Models using Only...</td>\n",
       "      <td>http://arxiv.org/abs/2405.14517v1</td>\n",
       "      <td>The widespread usage of large-scale multimodal...</td>\n",
       "      <td>[cs.LG, cs.CR]</td>\n",
       "      <td>[Songze Li, Ruoxi Cheng, Xiaojun Jia]</td>\n",
       "      <td>2024-05-23T12:54:25Z</td>\n",
       "      <td>CLIP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tuning-free Universally-Supervised Semantic Se...</td>\n",
       "      <td>http://arxiv.org/abs/2405.14294v1</td>\n",
       "      <td>This work presents a tuning-free semantic segm...</td>\n",
       "      <td>[cs.CV]</td>\n",
       "      <td>[Xiaobo Yang, Xiaojin Gong]</td>\n",
       "      <td>2024-05-23T08:13:52Z</td>\n",
       "      <td>CLIP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RET-CLIP: A Retinal Image Foundation Model Pre...</td>\n",
       "      <td>http://arxiv.org/abs/2405.14137v1</td>\n",
       "      <td>The Vision-Language Foundation model is increa...</td>\n",
       "      <td>[cs.CV]</td>\n",
       "      <td>[Jiawei Du, Jia Guo, Weihang Zhang, Shengzhu Y...</td>\n",
       "      <td>2024-05-23T03:20:51Z</td>\n",
       "      <td>CLIP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A Lost Opportunity for Vision-Language Models:...</td>\n",
       "      <td>http://arxiv.org/abs/2405.14977v1</td>\n",
       "      <td>In the realm of deep learning, maintaining mod...</td>\n",
       "      <td>[cs.CV]</td>\n",
       "      <td>[Mario Döbler, Robert A. Marsden, Tobias Raich...</td>\n",
       "      <td>2024-05-23T18:27:07Z</td>\n",
       "      <td>CLIP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Designing A Sustainable Marine Debris Clean-up...</td>\n",
       "      <td>http://arxiv.org/abs/2405.14815v1</td>\n",
       "      <td>Marine debris poses a significant ecological t...</td>\n",
       "      <td>[cs.CV, I.4; H.4; J.6]</td>\n",
       "      <td>[Raymond Wang, Nicholas R. Record, D. Whitney ...</td>\n",
       "      <td>2024-05-23T17:28:23Z</td>\n",
       "      <td>CLIP</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Identity Inference from CLIP Models using Only...   \n",
       "1  Tuning-free Universally-Supervised Semantic Se...   \n",
       "2  RET-CLIP: A Retinal Image Foundation Model Pre...   \n",
       "3  A Lost Opportunity for Vision-Language Models:...   \n",
       "4  Designing A Sustainable Marine Debris Clean-up...   \n",
       "\n",
       "                                link  \\\n",
       "0  http://arxiv.org/abs/2405.14517v1   \n",
       "1  http://arxiv.org/abs/2405.14294v1   \n",
       "2  http://arxiv.org/abs/2405.14137v1   \n",
       "3  http://arxiv.org/abs/2405.14977v1   \n",
       "4  http://arxiv.org/abs/2405.14815v1   \n",
       "\n",
       "                                            abstract                subjects  \\\n",
       "0  The widespread usage of large-scale multimodal...          [cs.LG, cs.CR]   \n",
       "1  This work presents a tuning-free semantic segm...                 [cs.CV]   \n",
       "2  The Vision-Language Foundation model is increa...                 [cs.CV]   \n",
       "3  In the realm of deep learning, maintaining mod...                 [cs.CV]   \n",
       "4  Marine debris poses a significant ecological t...  [cs.CV, I.4; H.4; J.6]   \n",
       "\n",
       "                                             authors             published  \\\n",
       "0              [Songze Li, Ruoxi Cheng, Xiaojun Jia]  2024-05-23T12:54:25Z   \n",
       "1                        [Xiaobo Yang, Xiaojin Gong]  2024-05-23T08:13:52Z   \n",
       "2  [Jiawei Du, Jia Guo, Weihang Zhang, Shengzhu Y...  2024-05-23T03:20:51Z   \n",
       "3  [Mario Döbler, Robert A. Marsden, Tobias Raich...  2024-05-23T18:27:07Z   \n",
       "4  [Raymond Wang, Nicholas R. Record, D. Whitney ...  2024-05-23T17:28:23Z   \n",
       "\n",
       "  label  \n",
       "0  CLIP  \n",
       "1  CLIP  \n",
       "2  CLIP  \n",
       "3  CLIP  \n",
       "4  CLIP  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6fff5981-588f-4302-b2fb-62c234be85c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import heapq\n",
    "from time import sleep\n",
    "from transformers import StoppingCriteria, StoppingCriteriaList\n",
    "\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer, PreTrainedTokenizerFast\n",
    "from datasets import load_dataset,load_from_disk\n",
    "from collections import defaultdict\n",
    "import pickle, json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2b345a1-c300-45f2-a7e0-ea65c0f8106d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8e0cfa6d53043529f5733ce7fdecb84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good, load successful!\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "save_dir = '/root/autodl-fs/llama-3-8b-instruct'\n",
    "\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(save_dir, low_cpu_mem_usage=True)\n",
    "model = LlamaForCausalLM.from_pretrained(save_dir, low_cpu_mem_usage=True, torch_dtype=torch.bfloat16)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.config.pad_token_id = model.config.eos_token_id\n",
    "model.cuda()\n",
    "print(\"Good, load successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17977ecb-760f-4d9b-ad7e-af8bfae3cb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PriorityQueue:\n",
    "    def __init__(self, max_size):\n",
    "        self.queue = []\n",
    "        self.max_size = max_size\n",
    "\n",
    "    def push(self, item):\n",
    "        try:\n",
    "            if len(self.queue) == self.max_size:\n",
    "                if item[0] > self.queue[0][0]:\n",
    "                    removed_item = heapq.heappop(self.queue)\n",
    "                    # print(f'Removed item with lower priority: {removed_item}')\n",
    "                else:\n",
    "                    return\n",
    "            # print(f'Pushing item: {item}')\n",
    "            heapq.heappush(self.queue, item)\n",
    "        except TypeError as e:\n",
    "            print(f\"TypeError encountered: {e}. Item: {item}\")\n",
    "            raise e\n",
    "\n",
    "    def pop(self):\n",
    "        return heapq.heappop(self.queue)\n",
    "\n",
    "    def peek(self):\n",
    "        return self.queue[0]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.queue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde45871-4d21-4908-868b-3566bd9afd77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "81f4a9e0-e4ec-4ade-9489-cf537b2ed373",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Related to CLIP\n",
      "0.7549149990081787 ATTENTION\n",
      "0.7549149990081787 GNN\n",
      "0.7879312038421631 CLIP\n",
      "0.798186719417572 CLIP\n",
      "0.8175744414329529 CLIP\n",
      "0.8175744414329529 CLIP\n",
      "0.8354834914207458 CLIP\n",
      "0.8438951373100281 CLIP\n",
      "0.851952850818634 CLIP\n",
      "0.8596637845039368 CLIP\n",
      "\n",
      "Related to Attention\n",
      "0.7772998213768005 ATTENTION\n",
      "0.7772998809814453 RLHF\n",
      "0.7879312038421631 ATTENTION\n",
      "0.7981867790222168 ATTENTION\n",
      "0.8080672025680542 ATTENTION\n",
      "0.8080672025680542 ATTENTION\n",
      "0.808067262172699 ATTENTION\n",
      "0.8175745010375977 ATTENTION\n",
      "0.8175745010375977 ATTENTION\n",
      "0.8354835510253906 GNN\n",
      "\n",
      "Related to GNN\n",
      "0.7662936449050903 GNN\n",
      "0.7981867790222168 GNN\n",
      "0.7981868386268616 GNN\n",
      "0.8080672025680542 GNN\n",
      "0.8267117738723755 GNN\n",
      "0.8267117738723755 GNN\n",
      "0.8267118334770203 GNN\n",
      "0.8519527912139893 GNN\n",
      "0.8519527912139893 GNN\n",
      "0.8519527912139893 GNN\n",
      "\n",
      "Related to RLHF\n",
      "0.7549149990081787 ATTENTION\n",
      "0.7662936449050903 RLHF\n",
      "0.7879312038421631 RLHF\n",
      "0.7879312038421631 RLHF\n",
      "0.7981867790222168 RLHF\n",
      "0.8080672025680542 RLHF\n",
      "0.8080672025680542 RLHF\n",
      "0.808067262172699 RLHF\n",
      "0.8267117738723755 RLHF\n",
      "0.8354834318161011 RLHF\n",
      "\n",
      "Related to optimization question\n",
      "0.7310585975646973 OPTIMIZATION\n",
      "0.7310585975646973 GNN\n",
      "0.7310585975646973 CLIP\n",
      "0.7310585975646973 ATTENTION\n",
      "0.7549149394035339 OPTIMIZATION\n",
      "0.7549149394035339 OPTIMIZATION\n",
      "0.7662936449050903 OPTIMIZATION\n",
      "0.7772998809814453 OPTIMIZATION\n",
      "0.7772998809814453 OPTIMIZATION\n",
      "0.7879311442375183 OPTIMIZATION\n"
     ]
    }
   ],
   "source": [
    "for filed in [\"Related to CLIP\",\"Related to Attention\",\"Related to GNN\",\"Related to RLHF\",\"Related to optimization question\"]:\n",
    "    demand = filed\n",
    "    print(\"\\n\"+demand)\n",
    "\n",
    "    df = pd.read_json('verification.json')\n",
    "    pq = PriorityQueue(max_size=10)\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        item = (row['title'], row['label'])\n",
    "        abstract = row['abstract']\n",
    "        prompt = \"Please judge that if this paper relates with all my demand: \\n ##Abstract##\\n{\\n\"\n",
    "        prompt = prompt + abstract + \"\\n}\\n\"\n",
    "        prompt = prompt + \"##Demands##\\n\"\n",
    "        prompt = prompt + demand\n",
    "        prompt = prompt + \"Does this paper relate to all my demands. Please answer directly Yes or No: Yes\"\n",
    "\n",
    "        input_ids = tokenizer(prompt, padding=True, return_tensors=\"pt\").input_ids.to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids)\n",
    "            logits = outputs.logits\n",
    "            probs = torch.log_softmax(logits, dim=-1)    \n",
    "        answer_ids = tokenizer([\"Yes\", \"No\"], return_tensors=\"pt\").input_ids.to(device)\n",
    "        last = probs[:,-2:-1,:]\n",
    "        # print(last.shape)\n",
    "        answer = answer_ids[:,-1].unsqueeze(0).unsqueeze(0)\n",
    "        # print(answer.shape)\n",
    "        gathered_probs = torch.gather(last, 2, answer)  \n",
    "        gathered_probs = gathered_probs.detach().cpu()\n",
    "        # print(\"Gathered probabilities:\", gathered_probs)\n",
    "        g_probs = np.array(gathered_probs.view(-1).detach())\n",
    "        exp_logits = np.exp(g_probs)\n",
    "        sum_exp_logits = np.sum(exp_logits)\n",
    "        probabilities = exp_logits / sum_exp_logits\n",
    "        # print(\"Yes prob: \", probabilities[0])\n",
    "        yes_prob = float(probabilities[0])\n",
    "        # item[\"prob\"] = yes_prob\n",
    "        # print((yes_prob, item))\n",
    "        pq.push((yes_prob, item))\n",
    "        \n",
    "    while pq.queue:\n",
    "        prob, item = pq.pop()\n",
    "        print(prob,item[1])\n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
